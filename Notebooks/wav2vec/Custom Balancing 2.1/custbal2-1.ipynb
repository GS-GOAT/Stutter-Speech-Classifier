{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-28T11:07:18.310171Z",
     "iopub.status.busy": "2025-07-28T11:07:18.309881Z",
     "iopub.status.idle": "2025-07-28T11:07:56.483484Z",
     "shell.execute_reply": "2025-07-28T11:07:56.482501Z",
     "shell.execute_reply.started": "2025-07-28T11:07:18.310134Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 11:07:31.610571: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753700851.802070      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753700851.855256      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting audiomentations\n",
      "  Downloading audiomentations-0.42.0-py3-none-any.whl.metadata (11 kB)\n",
      "Downloading audiomentations-0.42.0-py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.5/86.5 kB\u001b[0m \u001b[31m648.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: audiomentations\n",
      "Successfully installed audiomentations-0.42.0\n",
      "Collecting numpy_minmax\n",
      "  Downloading numpy_minmax-0.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting numpy_rms\n",
      "  Downloading numpy_rms-0.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
      "Collecting python_stretch\n",
      "  Downloading python_stretch-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from numpy_minmax) (1.17.1)\n",
      "Collecting numpy<3,>=2 (from numpy_minmax)\n",
      "  Downloading numpy-2.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m719.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.0->numpy_minmax) (2.22)\n",
      "Downloading numpy_minmax-0.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Downloading numpy_rms-0.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17 kB)\n",
      "Downloading python_stretch-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (113 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.4/113.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: python_stretch, numpy, numpy_rms, numpy_minmax\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.2 which is incompatible.\n",
      "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.3 which is incompatible.\n",
      "mkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.2 which is incompatible.\n",
      "mkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.2 which is incompatible.\n",
      "mkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.2 which is incompatible.\n",
      "cupy-cuda12x 13.4.1 requires numpy<2.3,>=1.22, but you have numpy 2.3.2 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.2 which is incompatible.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\n",
      "ydata-profiling 4.16.1 requires numpy<2.2,>=1.16.0, but you have numpy 2.3.2 which is incompatible.\n",
      "onnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "pandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.2 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.3.2 numpy_minmax-0.5.0 numpy_rms-0.6.0 python_stretch-0.3.1\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ==============================================================================\n",
    "# PRE-COMPUTATION WITH CUSTOM AUGMENTATION\n",
    "# ==============================================================================\n",
    "# This script uses my custom fine-tuned augmentation strategy to\n",
    "# create the final pre-computed feature set. It involved solving a optimization \n",
    "# problem that we want to increase the no. of clips in such a way that the mean  \n",
    "# is closest to without augment max class clips while making sure the balanced  \n",
    "# distribution has as less std deviation as possible. \n",
    "# As the clips are multi-lable, simply repeating clips might also lead to increase  \n",
    "# in no.s of max class which we want to leave as is.\n",
    "## THIS IS CUSTBAL2.1 : The Groups are divided properly for more balanced,\n",
    "## 70-15-15 split.\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import joblib\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "import torch\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "\n",
    "!pip install --no-deps audiomentations\n",
    "!pip install numpy_minmax numpy_rms python_stretch\n",
    "from audiomentations import Compose, AddGaussianNoise, PitchShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-28T11:08:20.779783Z",
     "iopub.status.busy": "2025-07-28T11:08:20.779185Z",
     "iopub.status.idle": "2025-07-28T11:08:20.785855Z",
     "shell.execute_reply": "2025-07-28T11:08:20.784989Z",
     "shell.execute_reply.started": "2025-07-28T11:08:20.779761Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "CLIPS_DIR = Path(\"/kaggle/input/sep-28k/clips/stuttering-clips/clips\")\n",
    "LABEL_FILE = \"/kaggle/input/sep-28k/SEP-28k_labels.csv\"\n",
    "OUTPUT_DIR = Path(\"./output_wav2vec_custom_augmented\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"facebook/wav2vec2-base-960h\"\n",
    "SAMPLING_RATE = 16000\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "MAX_SEQ_LEN = 150\n",
    "EMBEDDING_DIM = 768\n",
    "\n",
    "# Labels\n",
    "LABEL_COLS = ['Prolongation', 'Block', 'SoundRep', 'WordRep', 'Interjection']\n",
    "ALL_LABELS = LABEL_COLS + ['NoStutter']\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "CLEANED_DF_PATH = OUTPUT_DIR / \"df_multilabel.parquet\"\n",
    "TRAIN_DATA_PATH = OUTPUT_DIR / \"train_data.npz\"\n",
    "VAL_DATA_PATH = OUTPUT_DIR / \"val_data.npz\"\n",
    "TEST_DATA_PATH = OUTPUT_DIR / \"test_data.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-28T11:08:23.724203Z",
     "iopub.status.busy": "2025-07-28T11:08:23.723383Z",
     "iopub.status.idle": "2025-07-28T11:10:10.370398Z",
     "shell.execute_reply": "2025-07-28T11:10:10.369797Z",
     "shell.execute_reply.started": "2025-07-28T11:08:23.724176Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preparing and Splitting Data... ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e3256c224a4be7a9ca3f79db50a752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"--- Preparing and Splitting Data... ---\")\n",
    "if os.path.exists(CLEANED_DF_PATH):\n",
    "    df = pd.read_parquet(CLEANED_DF_PATH)\n",
    "else:\n",
    "    df = pd.read_csv(LABEL_FILE)\n",
    "    df['Clip'] = df['Show'].astype(str) + '_' + df['EpId'].astype(str) + '_' + df['ClipId'].astype(str)\n",
    "    clips_in_folder = {c.stem for c in CLIPS_DIR.glob(\"*.wav\")}\n",
    "    df = df[df['Clip'].isin(clips_in_folder)].copy()\n",
    "    def get_duration(clip_name):\n",
    "        try: return librosa.get_duration(path=CLIPS_DIR / f\"{clip_name}.wav\")\n",
    "        except Exception: return None\n",
    "    df['duration'] = df['Clip'].progress_apply(get_duration)\n",
    "    df.dropna(subset=['duration'], inplace=True)\n",
    "    df = df[(df['duration'] > 2.95) & (df['duration'] < 3.05)].copy()\n",
    "    for col in LABEL_COLS:\n",
    "        df[col] = (df[col] > 0).astype(int)\n",
    "    df['NoStutter'] = (df[LABEL_COLS].sum(axis=1) == 0).astype(int)\n",
    "    df.to_parquet(CLEANED_DF_PATH)\n",
    "\n",
    "df['speaker_id'] = df['Show']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-28T11:10:10.371775Z",
     "iopub.status.busy": "2025-07-28T11:10:10.371520Z",
     "iopub.status.idle": "2025-07-28T11:10:10.390719Z",
     "shell.execute_reply": "2025-07-28T11:10:10.390132Z",
     "shell.execute_reply.started": "2025-07-28T11:10:10.371756Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 18544\n",
      "Validation set size: 4300\n",
      "Test set size: 5056\n",
      "Data splitting complete using manual splits.\n"
     ]
    }
   ],
   "source": [
    "# manual splits\n",
    "manual_splits = {\n",
    "    'train': ['IStutterSoWhat', 'MyStutteringLife', 'StrongVoices', 'StutteringIsCool', 'WomenWhoStutter'],\n",
    "    'val': ['HeStutters','HVSA'],\n",
    "    'test': ['StutterTalk']\n",
    "}\n",
    "\n",
    "# DataFrames creation\n",
    "train_df = df[df['speaker_id'].isin(manual_splits['train'])].copy()\n",
    "val_df = df[df['speaker_id'].isin(manual_splits['val'])].copy()\n",
    "test_df = df[df['speaker_id'].isin(manual_splits['test'])].copy()\n",
    "\n",
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "print(\"Data splitting complete using manual splits.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-28T11:10:10.391696Z",
     "iopub.status.busy": "2025-07-28T11:10:10.391403Z",
     "iopub.status.idle": "2025-07-28T11:10:10.401749Z",
     "shell.execute_reply": "2025-07-28T11:10:10.401043Z",
     "shell.execute_reply.started": "2025-07-28T11:10:10.391668Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Label Distribution in Training Set (Before Augmentation) \n",
      "Block           7998\n",
      "Interjection    5909\n",
      "Prolongation    5569\n",
      "NoStutter       4162\n",
      "SoundRep        3448\n",
      "WordRep         2752\n",
      "dtype: int64\n",
      "\n",
      "--- Original Label Distribution in Val Set \n",
      "Interjection    1828\n",
      "Block           1809\n",
      "Prolongation    1522\n",
      "SoundRep        1193\n",
      "WordRep          914\n",
      "NoStutter        624\n",
      "dtype: int64\n",
      "\n",
      "--- Original Label Distribution in Test Set\n",
      "Block           2024\n",
      "Interjection    1859\n",
      "Prolongation    1353\n",
      "NoStutter       1150\n",
      "WordRep          929\n",
      "SoundRep         902\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# distribution before augmentation \n",
    "print(\"\\nOriginal Label Distribution in Training Set (Before Augmentation) \")\n",
    "print(train_df[ALL_LABELS].sum().sort_values(ascending=False))\n",
    "print(\"\\n--- Original Label Distribution in Val Set \")\n",
    "print(val_df[ALL_LABELS].sum().sort_values(ascending=False))\n",
    "print(\"\\n--- Original Label Distribution in Test Set\")\n",
    "print(test_df[ALL_LABELS].sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-28T11:12:26.250474Z",
     "iopub.status.busy": "2025-07-28T11:12:26.249717Z",
     "iopub.status.idle": "2025-07-28T11:38:49.459069Z",
     "shell.execute_reply": "2025-07-28T11:38:49.458339Z",
     "shell.execute_reply.started": "2025-07-28T11:12:26.250446Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Wav2Vec2 sequence extraction... ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f2adc2b25746f3a2244c14206008f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "802564b1a97640f1833a15f1cb95322f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc41447658d4749b1e71a6dd32369c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac2fd85ca084e55a748312312a0fcf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "496ac856b4314fab919c1df5b3cf3c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7632c4ad3ebf40f29bbe3d3e70ca4df1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab55e894b3c4bf19a72118375ab36ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating final size:   0%|          | 0/18544 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset size for 'Training' will be 33945 samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa98c746b87e4ba89bbd471550ba5f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved compressed training data with shape: (33945, 150, 768)\n",
      "\n",
      "--- Final Label Distribution in Training Set (After Augmentation) ---\n",
      "Block           10848\n",
      "Interjection    10824\n",
      "Prolongation    10800\n",
      "SoundRep        10576\n",
      "WordRep         10569\n",
      "NoStutter        4162\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset size for 'Validation' will be 4300 samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f977b6b1ae40e6a07307aaa92a2ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches:   0%|          | 0/135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved compressed validation data with shape: (4300, 150, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset size for 'Test' will be 5056 samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e2798e8ede4489ac9e88fe15c73904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Batches:   0%|          | 0/158 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved compressed test data with shape: (5056, 150, 768)\n",
      "\n",
      "--- Feature extraction complete. All files saved in 'output_wav2vec_custom_augmented' directory. ---\n"
     ]
    }
   ],
   "source": [
    "def extract_wav2vec_sequences(dataframe, desc=\"Extracting\", augment=False):\n",
    "    processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "    model = Wav2Vec2Model.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "    model.eval()\n",
    "    augmenter = Compose([\n",
    "        AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "        PitchShift(min_semitones=-4, max_semitones=4, p=0.5)\n",
    "    ])\n",
    "    \n",
    "    # Pre-calculate the final size of the dataset \n",
    "    final_size = 0\n",
    "    if augment:\n",
    "        for index, row in tqdm(dataframe.iterrows(), total=len(dataframe), desc=\"Calculating final size\"):\n",
    "            final_size += 1 \n",
    "            \n",
    "            # CUSTOM AUGMENTATION LOGIC \n",
    "            num_augmentations = 0\n",
    "            if row['Block'] == 0:\n",
    "                if row['WordRep'] == 1 or row['SoundRep'] == 1:\n",
    "                    num_augmentations = 3\n",
    "                elif row['Prolongation'] == 1:\n",
    "                    num_augmentations = 1\n",
    "            elif row['Block'] == 1:\n",
    "                if row['SoundRep'] == 1:\n",
    "                    num_augmentations = 1\n",
    "                elif row['WordRep'] == 1:\n",
    "                    num_augmentations = 1\n",
    "            \n",
    "            if row[LABEL_COLS].sum() == 1 and row['WordRep'] == 1:\n",
    "                num_augmentations += 3\n",
    "            \n",
    "            if row[LABEL_COLS].sum() == 2 and row['WordRep'] == 1 and row['SoundRep'] == 1:\n",
    "                num_augmentations += 2\n",
    "                \n",
    "            if row[LABEL_COLS].sum() == 1 and row['SoundRep'] == 1:\n",
    "                num_augmentations += 2\n",
    "            \n",
    "            final_size += num_augmentations\n",
    "    else:\n",
    "        final_size = len(dataframe)\n",
    "    \n",
    "    print(f\"Final dataset size for '{desc}' will be {final_size} samples.\")\n",
    "\n",
    "    # Pre-allocate NumPy arrays to save memory\n",
    "    all_sequences = np.zeros((final_size, MAX_SEQ_LEN, EMBEDDING_DIM), dtype=np.float32)\n",
    "    all_labels = np.zeros((final_size, len(ALL_LABELS)), dtype=np.int8)\n",
    "    \n",
    "    current_idx = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(dataframe), BATCH_SIZE), desc=f\"{desc} Batches\"):\n",
    "            batch_df = dataframe.iloc[i:i+BATCH_SIZE]\n",
    "            \n",
    "            audio_batch = []\n",
    "            labels_batch = []\n",
    "            \n",
    "            for index, row in batch_df.iterrows():\n",
    "                clip_path = str(CLIPS_DIR / f\"{row['Clip']}.wav\")\n",
    "                audio, sr = librosa.load(clip_path, sr=SAMPLING_RATE)\n",
    "                current_labels = row[ALL_LABELS].values\n",
    "                \n",
    "                audio_batch.append(audio)\n",
    "                labels_batch.append(current_labels)\n",
    "\n",
    "                if augment:\n",
    "                    # CUSTOM AUGMENTATION LOGIC \n",
    "                    num_augmentations = 0\n",
    "                    if row['Block'] == 0:\n",
    "                        if row['WordRep'] == 1 or row['SoundRep'] == 1:\n",
    "                            num_augmentations = 3\n",
    "                        elif row['Prolongation'] == 1:\n",
    "                            num_augmentations = 1\n",
    "                    elif row['Block'] == 1:\n",
    "                        if row['SoundRep'] == 1:\n",
    "                            num_augmentations = 1\n",
    "                        elif row['WordRep'] == 1:\n",
    "                            num_augmentations = 1\n",
    "\n",
    "                    if row[LABEL_COLS].sum() == 1 and row['WordRep'] == 1:\n",
    "                        num_augmentations += 3\n",
    "                    \n",
    "                    if row[LABEL_COLS].sum() == 2 and row['WordRep'] == 1 and row['SoundRep'] == 1:\n",
    "                        num_augmentations += 2\n",
    "\n",
    "                    if row[LABEL_COLS].sum() == 1 and row['SoundRep'] == 1:\n",
    "                        num_augmentations += 2\n",
    "\n",
    "                    for _ in range(num_augmentations):\n",
    "                        augmented_audio = augmenter(samples=audio, sample_rate=SAMPLING_RATE)\n",
    "                        audio_batch.append(augmented_audio)\n",
    "                        labels_batch.append(current_labels)\n",
    "\n",
    "            inputs = processor(audio_batch, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\", padding=True)\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            sequences = model(**inputs).last_hidden_state.cpu().numpy()\n",
    "            \n",
    "            # Fills the pre-allocated arrays\n",
    "            for j, seq in enumerate(sequences):\n",
    "                if current_idx >= final_size: break\n",
    "                if seq.shape[0] < MAX_SEQ_LEN:\n",
    "                    pad_width = MAX_SEQ_LEN - seq.shape[0]\n",
    "                    seq = np.pad(seq, ((0, pad_width), (0, 0)), mode='constant')\n",
    "                else:\n",
    "                    seq = seq[:MAX_SEQ_LEN, :]\n",
    "                \n",
    "                all_sequences[current_idx] = seq\n",
    "                all_labels[current_idx] = labels_batch[j]\n",
    "                current_idx += 1\n",
    "            if current_idx >= final_size: break\n",
    "\n",
    "    return all_sequences, all_labels\n",
    "\n",
    "print(\"\\n--- Starting Wav2Vec2 sequence extraction... ---\")\n",
    "X_train, y_train = extract_wav2vec_sequences(train_df, desc=\"Training\", augment=True)\n",
    "np.savez_compressed(TRAIN_DATA_PATH, x=X_train, y=y_train)\n",
    "print(f\"Saved compressed training data with shape: {X_train.shape}\")\n",
    "\n",
    "# distribution AFTER augmentation \n",
    "final_train_labels_df = pd.DataFrame(y_train, columns=ALL_LABELS)\n",
    "print(\"\\n--- Final Label Distribution in Training Set (After Augmentation) ---\")\n",
    "print(final_train_labels_df.sum().sort_values(ascending=False))\n",
    "\n",
    "# Clear memory before processing the next set for memory-constrained environments \n",
    "del X_train, y_train\n",
    "\n",
    "X_val, y_val = extract_wav2vec_sequences(val_df, desc=\"Validation\", augment=False)\n",
    "np.savez_compressed(VAL_DATA_PATH, x=X_val, y=y_val)\n",
    "print(f\"Saved compressed validation data with shape: {X_val.shape}\")\n",
    "del X_val, y_val\n",
    "\n",
    "X_test, y_test = extract_wav2vec_sequences(test_df, desc=\"Test\", augment=False)\n",
    "np.savez_compressed(TEST_DATA_PATH, x=X_test, y=y_test)\n",
    "print(f\"Saved compressed test data with shape: {X_test.shape}\")\n",
    "del X_test, y_test\n",
    "\n",
    "print(f\"\\n--- Feature extraction complete. All files saved in '{OUTPUT_DIR}' directory. ---\")\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "datasetId": 2285103,
     "sourceId": 3839191,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31091,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
