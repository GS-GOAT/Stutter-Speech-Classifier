{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-25T06:57:04.499804Z",
     "iopub.status.busy": "2025-07-25T06:57:04.499579Z",
     "iopub.status.idle": "2025-07-25T06:57:04.514703Z",
     "shell.execute_reply": "2025-07-25T06:57:04.514097Z",
     "shell.execute_reply.started": "2025-07-25T06:57:04.499777Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PRE-COMPUTATION WITH CUSTOM AUGMENTATION\n",
    "# ==============================================================================\n",
    "# This script uses my custom fine-tuned augmentation strategy to\n",
    "# create the final pre-computed feature set. It involved solving a optimization \n",
    "# problem that we want to increase the no. of clips in such a way that the mean  \n",
    "# is closest to without augment max class clips while making sure the balanced  \n",
    "# distribution has as less std deviation as possible. \n",
    "# As the clips are multi-lable, simply repeating clips might also lead to increase  \n",
    "# in no.s of max class which we want to leave as is.\n",
    "## THIS IS CUSTBAL2, the the DATASET IS MORE balanced than CUSTBAL1. Changes made \n",
    "## in custom balancing strategy.\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import joblib\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "import torch\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-25T06:57:04.515862Z",
     "iopub.status.busy": "2025-07-25T06:57:04.515617Z",
     "iopub.status.idle": "2025-07-25T06:57:09.075730Z",
     "shell.execute_reply": "2025-07-25T06:57:09.074493Z",
     "shell.execute_reply.started": "2025-07-25T06:57:04.515818Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: audiomentations in /usr/local/lib/python3.11/dist-packages (0.42.0)\n",
      "Requirement already satisfied: numpy_minmax in /usr/local/lib/python3.11/dist-packages (0.5.0)\n",
      "Requirement already satisfied: numpy_rms in /usr/local/lib/python3.11/dist-packages (0.6.0)\n",
      "Requirement already satisfied: python_stretch in /usr/local/lib/python3.11/dist-packages (0.3.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from numpy_minmax) (1.17.1)\n",
      "Requirement already satisfied: numpy<3,>=2 in /usr/local/lib/python3.11/dist-packages (from numpy_minmax) (2.3.2)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.0->numpy_minmax) (2.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-deps audiomentations\n",
    "!pip install numpy_minmax numpy_rms python_stretch\n",
    "from audiomentations import Compose, AddGaussianNoise, PitchShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T06:58:10.089203Z",
     "iopub.status.busy": "2025-07-25T06:58:10.088883Z",
     "iopub.status.idle": "2025-07-25T06:58:10.244108Z",
     "shell.execute_reply": "2025-07-25T06:58:10.243097Z",
     "shell.execute_reply.started": "2025-07-25T06:58:10.089175Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clips\t\t\t  fluencybank_labels.csv  SEP-28k_labels.csv\n",
      "fluencybank_episodes.csv  SEP-28k_episodes.csv\n"
     ]
    }
   ],
   "source": [
    "!ls ../input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-25T07:00:52.795768Z",
     "iopub.status.busy": "2025-07-25T07:00:52.795480Z",
     "iopub.status.idle": "2025-07-25T07:00:52.801606Z",
     "shell.execute_reply": "2025-07-25T07:00:52.800974Z",
     "shell.execute_reply.started": "2025-07-25T07:00:52.795750Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Configuration \n",
    "CLIPS_DIR = Path(\"/kaggle/input/clips/stuttering-clips/clips\")\n",
    "LABEL_FILE = \"/kaggle/input/SEP-28k_labels.csv\"\n",
    "OUTPUT_DIR = Path(\"./output_wav2vec_custom_augmented\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"facebook/wav2vec2-base-960h\"\n",
    "SAMPLING_RATE = 16000\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "MAX_SEQ_LEN = 150\n",
    "EMBEDDING_DIM = 768\n",
    "\n",
    "# Label Definitions \n",
    "LABEL_COLS = ['Prolongation', 'Block', 'SoundRep', 'WordRep', 'Interjection']\n",
    "ALL_LABELS = LABEL_COLS + ['NoStutter']\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "CLEANED_DF_PATH = OUTPUT_DIR / \"df_multilabel.parquet\"\n",
    "TRAIN_DATA_PATH = OUTPUT_DIR / \"train_data.npz\"\n",
    "VAL_DATA_PATH = OUTPUT_DIR / \"val_data.npz\"\n",
    "TEST_DATA_PATH = OUTPUT_DIR / \"test_data.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-25T07:00:56.322641Z",
     "iopub.status.busy": "2025-07-25T07:00:56.322084Z",
     "iopub.status.idle": "2025-07-25T07:03:38.637271Z",
     "shell.execute_reply": "2025-07-25T07:03:38.636399Z",
     "shell.execute_reply.started": "2025-07-25T07:00:56.322617Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preparing and Splitting Data... ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4628fa32c3d4c8ebe817632b6f425ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splitting complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Preparing and Splitting Data... ---\")\n",
    "if os.path.exists(CLEANED_DF_PATH):\n",
    "    df = pd.read_parquet(CLEANED_DF_PATH)\n",
    "else:\n",
    "    df = pd.read_csv(LABEL_FILE)\n",
    "    df['Clip'] = df['Show'].astype(str) + '_' + df['EpId'].astype(str) + '_' + df['ClipId'].astype(str)\n",
    "    clips_in_folder = {c.stem for c in CLIPS_DIR.glob(\"*.wav\")}\n",
    "    df = df[df['Clip'].isin(clips_in_folder)].copy()\n",
    "    def get_duration(clip_name):\n",
    "        try: return librosa.get_duration(path=CLIPS_DIR / f\"{clip_name}.wav\")\n",
    "        except Exception: return None\n",
    "    df['duration'] = df['Clip'].progress_apply(get_duration)\n",
    "    df.dropna(subset=['duration'], inplace=True)\n",
    "    df = df[(df['duration'] > 2.95) & (df['duration'] < 3.05)].copy()\n",
    "    for col in LABEL_COLS:\n",
    "        df[col] = (df[col] > 0).astype(int)\n",
    "    df['NoStutter'] = (df[LABEL_COLS].sum(axis=1) == 0).astype(int)\n",
    "    df.to_parquet(CLEANED_DF_PATH)\n",
    "\n",
    "df['speaker_id'] = df['Show']\n",
    "gss_test = GroupShuffleSplit(n_splits=1, test_size=0.15, random_state=42)\n",
    "train_val_idx, test_idx = next(gss_test.split(df, groups=df['speaker_id']))\n",
    "train_val_df = df.iloc[train_val_idx]\n",
    "test_df = df.iloc[test_idx]\n",
    "gss_val = GroupShuffleSplit(n_splits=1, test_size=0.15, random_state=42)\n",
    "train_idx, val_idx = next(gss_val.split(train_val_df, groups=train_val_df['speaker_id']))\n",
    "train_df = train_val_df.iloc[train_idx]\n",
    "val_df = train_val_df.iloc[val_idx]\n",
    "print(\"Data splitting complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-25T07:03:38.639126Z",
     "iopub.status.busy": "2025-07-25T07:03:38.638678Z",
     "iopub.status.idle": "2025-07-25T07:03:38.646440Z",
     "shell.execute_reply": "2025-07-25T07:03:38.645719Z",
     "shell.execute_reply.started": "2025-07-25T07:03:38.639104Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Original Label Distribution in Training Set (Before Augmentation) ---\n",
      "Block           7998\n",
      "Interjection    5909\n",
      "Prolongation    5569\n",
      "NoStutter       4162\n",
      "SoundRep        3448\n",
      "WordRep         2752\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# distribution BEFORE augmentation \n",
    "print(\"\\n--- Original Label Distribution in Training Set (Before Augmentation) ---\")\n",
    "print(train_df[ALL_LABELS].sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-25T07:03:38.647773Z",
     "iopub.status.busy": "2025-07-25T07:03:38.647211Z",
     "iopub.status.idle": "2025-07-25T07:30:30.966175Z",
     "shell.execute_reply": "2025-07-25T07:30:30.965353Z",
     "shell.execute_reply.started": "2025-07-25T07:03:38.647747Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Wav2Vec2 sequence extraction... ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13858d76fa2645059392cd5ef2a2d482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24b2fe9da3004a00849bb11fdd344a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0caf249e5ec64ec49f47bb2a190f1bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b569ee09831b48b191493aa5278465aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6332b371f9d24ad4842bf9ba5e12cd06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9976496bf8a4b8c9b104eafc1823f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af54afd33cbc432596b6b7d98135cdcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating final size:   0%|          | 0/18544 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset size for 'Training' will be 33945 samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a76d139d144a97ad923f0db968a2d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved compressed training data with shape: (33945, 150, 768)\n",
      "\n",
      "--- Final Label Distribution in Training Set (After Augmentation) ---\n",
      "Block           10848\n",
      "Interjection    10824\n",
      "Prolongation    10800\n",
      "SoundRep        10576\n",
      "WordRep         10569\n",
      "NoStutter        4162\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset size for 'Validation' will be 734 samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ca7e0daaea465caa7c3ec3ca062751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved compressed validation data with shape: (734, 150, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset size for 'Test' will be 8622 samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0156d499ee84160a668642bb58b5689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Batches:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved compressed test data with shape: (8622, 150, 768)\n",
      "\n",
      "--- Feature extraction complete. All files saved in 'output_wav2vec_custom_augmented' directory. ---\n"
     ]
    }
   ],
   "source": [
    "def extract_wav2vec_sequences(dataframe, desc=\"Extracting\", augment=False):\n",
    "    processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "    model = Wav2Vec2Model.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "    model.eval()\n",
    "    augmenter = Compose([\n",
    "        AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "        PitchShift(min_semitones=-4, max_semitones=4, p=0.5)\n",
    "    ])\n",
    "    \n",
    "    # Pre-calculate the final size of the dataset \n",
    "    final_size = 0\n",
    "    if augment:\n",
    "        for index, row in tqdm(dataframe.iterrows(), total=len(dataframe), desc=\"Calculating final size\"):\n",
    "            final_size += 1   \n",
    "            \n",
    "            # CUSTOM AUGMENTATION LOGIC \n",
    "            num_augmentations = 0\n",
    "            if row['Block'] == 0:\n",
    "                if row['WordRep'] == 1 or row['SoundRep'] == 1:\n",
    "                    num_augmentations = 3\n",
    "                elif row['Prolongation'] == 1:\n",
    "                    num_augmentations = 1\n",
    "            elif row['Block'] == 1:\n",
    "                if row['SoundRep'] == 1:\n",
    "                    num_augmentations = 1\n",
    "                elif row['WordRep'] == 1:\n",
    "                    num_augmentations = 1\n",
    "            \n",
    "            if row[LABEL_COLS].sum() == 1 and row['WordRep'] == 1:\n",
    "                num_augmentations += 3\n",
    "            \n",
    "            if row[LABEL_COLS].sum() == 2 and row['WordRep'] == 1 and row['SoundRep'] == 1:\n",
    "                num_augmentations += 2\n",
    "                \n",
    "            if row[LABEL_COLS].sum() == 1 and row['SoundRep'] == 1:\n",
    "                num_augmentations += 2\n",
    "            \n",
    "            final_size += num_augmentations\n",
    "    else:\n",
    "        final_size = len(dataframe)\n",
    "    \n",
    "    print(f\"Final dataset size for '{desc}' will be {final_size} samples.\")\n",
    "\n",
    "    # Pre-allocate NumPy arrays \n",
    "    all_sequences = np.zeros((final_size, MAX_SEQ_LEN, EMBEDDING_DIM), dtype=np.float32)\n",
    "    all_labels = np.zeros((final_size, len(ALL_LABELS)), dtype=np.int8)\n",
    "    \n",
    "    current_idx = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(dataframe), BATCH_SIZE), desc=f\"{desc} Batches\"):\n",
    "            batch_df = dataframe.iloc[i:i+BATCH_SIZE]\n",
    "            \n",
    "            audio_batch = []\n",
    "            labels_batch = []\n",
    "            \n",
    "            for index, row in batch_df.iterrows():\n",
    "                clip_path = str(CLIPS_DIR / f\"{row['Clip']}.wav\")\n",
    "                audio, sr = librosa.load(clip_path, sr=SAMPLING_RATE)\n",
    "                current_labels = row[ALL_LABELS].values\n",
    "                \n",
    "                audio_batch.append(audio)\n",
    "                labels_batch.append(current_labels)\n",
    "\n",
    "                if augment:\n",
    "                    # CUSTOM AUGMENTATION LOGIC \n",
    "                    num_augmentations = 0\n",
    "                    if row['Block'] == 0:\n",
    "                        if row['WordRep'] == 1 or row['SoundRep'] == 1:\n",
    "                            num_augmentations = 3\n",
    "                        elif row['Prolongation'] == 1:\n",
    "                            num_augmentations = 1\n",
    "                    elif row['Block'] == 1:\n",
    "                        if row['SoundRep'] == 1:\n",
    "                            num_augmentations = 1\n",
    "                        elif row['WordRep'] == 1:\n",
    "                            num_augmentations = 1\n",
    "\n",
    "                    if row[LABEL_COLS].sum() == 1 and row['WordRep'] == 1:\n",
    "                        num_augmentations += 3\n",
    "                    \n",
    "                    if row[LABEL_COLS].sum() == 2 and row['WordRep'] == 1 and row['SoundRep'] == 1:\n",
    "                        num_augmentations += 2\n",
    "\n",
    "                    if row[LABEL_COLS].sum() == 1 and row['SoundRep'] == 1:\n",
    "                        num_augmentations += 2\n",
    "\n",
    "                    for _ in range(num_augmentations):\n",
    "                        augmented_audio = augmenter(samples=audio, sample_rate=SAMPLING_RATE)\n",
    "                        audio_batch.append(augmented_audio)\n",
    "                        labels_batch.append(current_labels)\n",
    "\n",
    "            inputs = processor(audio_batch, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\", padding=True)\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            sequences = model(**inputs).last_hidden_state.cpu().numpy()\n",
    "            \n",
    "            # Fill the pre-allocated arrays\n",
    "            for j, seq in enumerate(sequences):\n",
    "                if current_idx >= final_size: break\n",
    "                if seq.shape[0] < MAX_SEQ_LEN:\n",
    "                    pad_width = MAX_SEQ_LEN - seq.shape[0]\n",
    "                    seq = np.pad(seq, ((0, pad_width), (0, 0)), mode='constant')\n",
    "                else:\n",
    "                    seq = seq[:MAX_SEQ_LEN, :]\n",
    "                \n",
    "                all_sequences[current_idx] = seq\n",
    "                all_labels[current_idx] = labels_batch[j]\n",
    "                current_idx += 1\n",
    "            if current_idx >= final_size: break\n",
    "\n",
    "    return all_sequences, all_labels\n",
    "\n",
    "print(\"\\n--- Starting Wav2Vec2 sequence extraction... ---\")\n",
    "X_train, y_train = extract_wav2vec_sequences(train_df, desc=\"Training\", augment=True)\n",
    "np.savez_compressed(TRAIN_DATA_PATH, x=X_train, y=y_train)\n",
    "print(f\"Saved compressed training data with shape: {X_train.shape}\")\n",
    "\n",
    "# distribution AFTER augmentation \n",
    "final_train_labels_df = pd.DataFrame(y_train, columns=ALL_LABELS)\n",
    "print(\"\\n--- Final Label Distribution in Training Set (After Augmentation) ---\")\n",
    "print(final_train_labels_df.sum().sort_values(ascending=False))\n",
    "\n",
    "# Clear memory before processing the next set for memory-constrained environments \n",
    "del X_train, y_train\n",
    "\n",
    "X_val, y_val = extract_wav2vec_sequences(val_df, desc=\"Validation\", augment=False)\n",
    "np.savez_compressed(VAL_DATA_PATH, x=X_val, y=y_val)\n",
    "print(f\"Saved compressed validation data with shape: {X_val.shape}\")\n",
    "del X_val, y_val\n",
    "\n",
    "X_test, y_test = extract_wav2vec_sequences(test_df, desc=\"Test\", augment=False)\n",
    "np.savez_compressed(TEST_DATA_PATH, x=X_test, y=y_test)\n",
    "print(f\"Saved compressed test data with shape: {X_test.shape}\")\n",
    "del X_test, y_test\n",
    "\n",
    "print(f\"\\n--- Feature extraction complete. All files saved in '{OUTPUT_DIR}' directory. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2285103,
     "sourceId": 3839191,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
