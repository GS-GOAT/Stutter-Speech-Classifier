{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-20T18:30:14.750560Z",
     "iopub.status.busy": "2025-07-20T18:30:14.749884Z",
     "iopub.status.idle": "2025-07-20T18:30:39.312485Z",
     "shell.execute_reply": "2025-07-20T18:30:39.311601Z",
     "shell.execute_reply.started": "2025-07-20T18:30:14.750539Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import joblib\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "import torch\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-20T18:30:39.314630Z",
     "iopub.status.busy": "2025-07-20T18:30:39.314011Z",
     "iopub.status.idle": "2025-07-20T18:30:50.160997Z",
     "shell.execute_reply": "2025-07-20T18:30:50.160258Z",
     "shell.execute_reply.started": "2025-07-20T18:30:39.314609Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --no-deps audiomentations\n",
    "!pip install numpy_minmax numpy_rms python_stretch\n",
    "from audiomentations import Compose, AddGaussianNoise, PitchShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-20T18:31:11.903395Z",
     "iopub.status.busy": "2025-07-20T18:31:11.903116Z",
     "iopub.status.idle": "2025-07-20T18:31:11.909320Z",
     "shell.execute_reply": "2025-07-20T18:31:11.908579Z",
     "shell.execute_reply.started": "2025-07-20T18:31:11.903374Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configuration \n",
    "CLIPS_DIR = Path(\"/kaggle/input/sep-28k/clips/stuttering-clips/clips\")\n",
    "LABEL_FILE = \"/kaggle/input/sep-28k/SEP-28k_labels.csv\"\n",
    "OUTPUT_DIR = Path(\"./output_wav2vec_precomputed_features\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"facebook/wav2vec2-base-960h\"\n",
    "SAMPLING_RATE = 16000\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "MAX_SEQ_LEN = 150\n",
    "EMBEDDING_DIM = 768\n",
    "\n",
    "LABEL_COLS = ['Prolongation', 'Block', 'SoundRep', 'WordRep', 'Interjection']\n",
    "ALL_LABELS = LABEL_COLS + ['NoStutter'] \n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "CLEANED_DF_PATH = OUTPUT_DIR / \"df_multilabel.parquet\"\n",
    "TRAIN_EMBEDDINGS_PATH, TRAIN_LABELS_PATH = OUTPUT_DIR / \"train_seq_embeddings.npy\", OUTPUT_DIR / \"train_labels.npy\"\n",
    "VAL_EMBEDDINGS_PATH, VAL_LABELS_PATH = OUTPUT_DIR / \"val_seq_embeddings.npy\", OUTPUT_DIR / \"val_labels.npy\"\n",
    "TEST_EMBEDDINGS_PATH, TEST_LABELS_PATH = OUTPUT_DIR / \"test_seq_embeddings.npy\", OUTPUT_DIR / \"test_labels.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-20T18:39:39.597953Z",
     "iopub.status.busy": "2025-07-20T18:39:39.597646Z",
     "iopub.status.idle": "2025-07-20T18:39:39.648821Z",
     "shell.execute_reply": "2025-07-20T18:39:39.648034Z",
     "shell.execute_reply.started": "2025-07-20T18:39:39.597931Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Preparing and Splitting Data\")\n",
    "if os.path.exists(CLEANED_DF_PATH):\n",
    "    print('FOUND')\n",
    "    df = pd.read_parquet(CLEANED_DF_PATH)\n",
    "else:\n",
    "    df = pd.read_csv(LABEL_FILE)\n",
    "    df['Clip'] = df['Show'].astype(str) + '_' + df['EpId'].astype(str) + '_' + df['ClipId'].astype(str)\n",
    "    clips_in_folder = {c.stem for c in CLIPS_DIR.glob(\"*.wav\")}\n",
    "    df = df[df['Clip'].isin(clips_in_folder)].copy()\n",
    "    def get_duration(clip_name):\n",
    "        try: return librosa.get_duration(path=CLIPS_DIR / f\"{clip_name}.wav\")\n",
    "        except Exception: return None\n",
    "    df['duration'] = df['Clip'].progress_apply(get_duration)\n",
    "    df.dropna(subset=['duration'], inplace=True)\n",
    "    df = df[(df['duration'] > 2.95) & (df['duration'] < 3.05)].copy()\n",
    "    for col in LABEL_COLS:\n",
    "        df[col] = (df[col] > 0).astype(int)\n",
    "    df['NoStutter'] = (df[LABEL_COLS].sum(axis=1) == 0).astype(int)\n",
    "    df.to_parquet(CLEANED_DF_PATH)\n",
    "\n",
    "df['speaker_id'] = df['Show']\n",
    "gss_test = GroupShuffleSplit(n_splits=1, test_size=0.15, random_state=42)\n",
    "train_val_idx, test_idx = next(gss_test.split(df, groups=df['speaker_id']))\n",
    "train_val_df = df.iloc[train_val_idx] \n",
    "test_df = df.iloc[test_idx]\n",
    "gss_val = GroupShuffleSplit(n_splits=1, test_size=0.15, random_state=42)\n",
    "train_idx, val_idx = next(gss_val.split(train_val_df, groups=train_val_df['speaker_id']))\n",
    "train_df = train_val_df.iloc[train_idx]\n",
    "val_df = train_val_df.iloc[val_idx]\n",
    "print(\"Data splitting complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-20T18:39:51.498972Z",
     "iopub.status.busy": "2025-07-20T18:39:51.498266Z",
     "iopub.status.idle": "2025-07-20T18:51:49.522458Z",
     "shell.execute_reply": "2025-07-20T18:51:49.521594Z",
     "shell.execute_reply.started": "2025-07-20T18:39:51.498950Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def extract_wav2vec_sequences(dataframe, desc=\"Extracting\", augment=False):\n",
    "    processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "    model = Wav2Vec2Model.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "    model.eval()\n",
    "    augmenter = Compose([\n",
    "        AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "        PitchShift(min_semitones=-4, max_semitones=4, p=0.5)\n",
    "    ])\n",
    "    \n",
    "    final_size = 0\n",
    "    if augment:\n",
    "        label_counts = dataframe[ALL_LABELS].sum()\n",
    "        max_count = label_counts.max()\n",
    "        for index, row in tqdm(dataframe.iterrows(), total=len(dataframe), desc=\"Calculating final size\"):\n",
    "            final_size += 1 \n",
    "            current_labels = row[ALL_LABELS].values\n",
    "            active_labels = [label for k, label in enumerate(ALL_LABELS) if current_labels[k] == 1]\n",
    "            if not active_labels: continue\n",
    "            rarest_label_count = min([label_counts[label] for label in active_labels])\n",
    "            num_augmentations = min(4, max(0, round((max_count / rarest_label_count) - 1)))\n",
    "            final_size += num_augmentations\n",
    "    else:\n",
    "        final_size = len(dataframe)\n",
    "    \n",
    "    print(f\"Final dataset size for '{desc}' will be {final_size} samples.\")\n",
    "\n",
    "    # Pre-allocate NumPy arrays\n",
    "    all_sequences = np.zeros((final_size, MAX_SEQ_LEN, EMBEDDING_DIM), dtype=np.float32)\n",
    "    all_labels = np.zeros((final_size, len(ALL_LABELS)), dtype=np.int8)\n",
    "    \n",
    "    current_idx = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(dataframe), BATCH_SIZE), desc=f\"{desc} Batches\"):\n",
    "            batch_df = dataframe.iloc[i:i+BATCH_SIZE]\n",
    "            \n",
    "            audio_batch = []\n",
    "            labels_batch = []\n",
    "            \n",
    "            for index, row in batch_df.iterrows():\n",
    "                clip_path = str(CLIPS_DIR / f\"{row['Clip']}.wav\")\n",
    "                audio, sr = librosa.load(clip_path, sr=SAMPLING_RATE)\n",
    "                current_labels = row[ALL_LABELS].values\n",
    "                \n",
    "                audio_batch.append(audio)\n",
    "                labels_batch.append(current_labels)\n",
    "\n",
    "                if augment:\n",
    "                    active_labels = [label for k, label in enumerate(ALL_LABELS) if current_labels[k] == 1]\n",
    "                    if not active_labels: continue\n",
    "                    rarest_label_count = min([label_counts[label] for label in active_labels])\n",
    "                    num_augmentations = min(4, max(0, round((max_count / rarest_label_count) - 1)))\n",
    "\n",
    "                    for _ in range(num_augmentations):\n",
    "                        augmented_audio = augmenter(samples=audio, sample_rate=SAMPLING_RATE)\n",
    "                        audio_batch.append(augmented_audio)\n",
    "                        labels_batch.append(current_labels)\n",
    "\n",
    "            inputs = processor(audio_batch, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\", padding=True)\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            sequences = model(**inputs).last_hidden_state.cpu().numpy()\n",
    "            \n",
    "            # Fill the pre-allocated arrays\n",
    "            for j, seq in enumerate(sequences):\n",
    "                if current_idx >= final_size: break\n",
    "                if seq.shape[0] < MAX_SEQ_LEN:\n",
    "                    pad_width = MAX_SEQ_LEN - seq.shape[0]\n",
    "                    seq = np.pad(seq, ((0, pad_width), (0, 0)), mode='constant')\n",
    "                else:\n",
    "                    seq = seq[:MAX_SEQ_LEN, :]\n",
    "                \n",
    "                all_sequences[current_idx] = seq\n",
    "                all_labels[current_idx] = labels_batch[j]\n",
    "                current_idx += 1\n",
    "            if current_idx >= final_size: break\n",
    "\n",
    "    return all_sequences, all_labels\n",
    "\n",
    "print(\"\\nStarting Wav2Vec2 sequence extraction\")\n",
    "X_train, y_train = extract_wav2vec_sequences(train_df, desc=\"Training\", augment=True)\n",
    "np.save(TRAIN_EMBEDDINGS_PATH, X_train); np.save(TRAIN_LABELS_PATH, y_train)\n",
    "print(f\"Saved training embeddings with shape: {X_train.shape}\")\n",
    "\n",
    "del X_train, y_train\n",
    "\n",
    "X_val, y_val = extract_wav2vec_sequences(val_df, desc=\"Validation\", augment=False)\n",
    "np.save(VAL_EMBEDDINGS_PATH, X_val); np.save(VAL_LABELS_PATH, y_val)\n",
    "print(f\"Saved validation embeddings with shape: {X_val.shape}\")\n",
    "del X_val, y_val\n",
    "\n",
    "X_test, y_test = extract_wav2vec_sequences(test_df, desc=\"Test\", augment=False)\n",
    "np.save(TEST_EMBEDDINGS_PATH, X_test); np.save(TEST_LABELS_PATH, y_test)\n",
    "print(f\"Saved test embeddings with shape: {X_test.shape}\")\n",
    "del X_test, y_test\n",
    "\n",
    "print(f\"\\nFeature extraction complete. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2285103,
     "sourceId": 3839191,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
