{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-29T02:27:15.910373Z",
     "iopub.status.busy": "2025-11-29T02:27:15.909843Z",
     "iopub.status.idle": "2025-11-29T02:28:03.711328Z",
     "shell.execute_reply": "2025-11-29T02:28:03.710357Z",
     "shell.execute_reply.started": "2025-11-29T02:27:15.910347Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import joblib\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "!pip install --upgrade transformers huggingface-hub --quiet\n",
    "import torch\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "\n",
    "!pip install --no-deps audiomentations --quiet\n",
    "!pip install numpy_minmax numpy_rms python_stretch --quiet \n",
    "from audiomentations import Compose, AddGaussianNoise, PitchShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T02:28:03.713853Z",
     "iopub.status.busy": "2025-11-29T02:28:03.712862Z",
     "iopub.status.idle": "2025-11-29T02:28:03.720669Z",
     "shell.execute_reply": "2025-11-29T02:28:03.719833Z",
     "shell.execute_reply.started": "2025-11-29T02:28:03.713823Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "CLIPS_DIR = Path(\"/kaggle/input/sep-28k/clips/stuttering-clips/clips\")\n",
    "LABEL_FILE = \"/kaggle/input/sep-28k/SEP-28k_labels.csv\"\n",
    "OUTPUT_DIR = Path(\"/kaggle/working/output_wav2vec_custom_augmented\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"facebook/wav2vec2-large-960h\"\n",
    "SAMPLING_RATE = 16000\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "MAX_SEQ_LEN = 150\n",
    "EMBEDDING_DIM = 1024\n",
    "\n",
    "# Labels\n",
    "LABEL_COLS = ['Prolongation', 'Block', 'SoundRep', 'WordRep', 'Interjection']\n",
    "ALL_LABELS = LABEL_COLS + ['NoStutter']\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "CLEANED_DF_PATH = OUTPUT_DIR / \"df_multilabel.parquet\"\n",
    "TRAIN_DATA_PATH = OUTPUT_DIR / \"train_data.npz\"\n",
    "VAL_DATA_PATH = OUTPUT_DIR / \"val_data.npz\"\n",
    "TEST_DATA_PATH = OUTPUT_DIR / \"test_data.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T02:28:03.721638Z",
     "iopub.status.busy": "2025-11-29T02:28:03.721395Z",
     "iopub.status.idle": "2025-11-29T02:29:55.167891Z",
     "shell.execute_reply": "2025-11-29T02:29:55.167080Z",
     "shell.execute_reply.started": "2025-11-29T02:28:03.721622Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\" Preparing and Splitting Data... \")\n",
    "if os.path.exists(CLEANED_DF_PATH):\n",
    "    df = pd.read_parquet(CLEANED_DF_PATH)\n",
    "else:\n",
    "    df = pd.read_csv(LABEL_FILE)\n",
    "    df['Clip'] = df['Show'].astype(str) + '_' + df['EpId'].astype(str) + '_' + df['ClipId'].astype(str)\n",
    "    clips_in_folder = {c.stem for c in CLIPS_DIR.glob(\"*.wav\")}\n",
    "    df = df[df['Clip'].isin(clips_in_folder)].copy()\n",
    "    def get_duration(clip_name):\n",
    "        try: return librosa.get_duration(path=CLIPS_DIR / f\"{clip_name}.wav\")\n",
    "        except Exception: return None\n",
    "    df['duration'] = df['Clip'].progress_apply(get_duration)\n",
    "    df.dropna(subset=['duration'], inplace=True)\n",
    "    df = df[(df['duration'] > 2.95) & (df['duration'] < 3.05)].copy()\n",
    "    for col in LABEL_COLS:\n",
    "        df[col] = (df[col] > 0).astype(int)\n",
    "    df['NoStutter'] = (df[LABEL_COLS].sum(axis=1) == 0).astype(int)\n",
    "    df.to_parquet(CLEANED_DF_PATH)\n",
    "\n",
    "df['speaker_id'] = df['Show']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T02:29:55.169796Z",
     "iopub.status.busy": "2025-11-29T02:29:55.169456Z",
     "iopub.status.idle": "2025-11-29T02:29:55.195052Z",
     "shell.execute_reply": "2025-11-29T02:29:55.194358Z",
     "shell.execute_reply.started": "2025-11-29T02:29:55.169754Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# manual splits\n",
    "manual_splits = {\n",
    "    'train': ['IStutterSoWhat', 'MyStutteringLife', 'StrongVoices', 'StutteringIsCool', 'WomenWhoStutter'],\n",
    "    'val': ['HeStutters','HVSA'],\n",
    "    'test': ['StutterTalk']\n",
    "}\n",
    "\n",
    "# DataFrames creation\n",
    "train_df = df[df['speaker_id'].isin(manual_splits['train'])].copy()\n",
    "val_df = df[df['speaker_id'].isin(manual_splits['val'])].copy()\n",
    "test_df = df[df['speaker_id'].isin(manual_splits['test'])].copy()\n",
    "\n",
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "print(\"Data splitting complete using manual splits.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T02:29:55.196161Z",
     "iopub.status.busy": "2025-11-29T02:29:55.195901Z",
     "iopub.status.idle": "2025-11-29T02:29:55.231814Z",
     "shell.execute_reply": "2025-11-29T02:29:55.231013Z",
     "shell.execute_reply.started": "2025-11-29T02:29:55.196139Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# distribution before augmentation \n",
    "print(\"\\nOriginal Label Distribution in Training Set (Before Augmentation) \")\n",
    "print(train_df[ALL_LABELS].sum().sort_values(ascending=False))\n",
    "print(\"\\n Original Label Distribution in Val Set \")\n",
    "print(val_df[ALL_LABELS].sum().sort_values(ascending=False))\n",
    "print(\"\\n Original Label Distribution in Test Set\")\n",
    "print(test_df[ALL_LABELS].sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-29T02:30:42.004Z",
     "iopub.execute_input": "2025-11-29T02:29:55.233050Z",
     "iopub.status.busy": "2025-11-29T02:29:55.232696Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_wav2vec_sequences(dataframe, desc=\"Extracting\", augment=False):\n",
    "    processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "    model = Wav2Vec2Model.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "    model.eval()\n",
    "    augmenter = Compose([\n",
    "        AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "        PitchShift(min_semitones=-4, max_semitones=4, p=0.5)\n",
    "    ])\n",
    "    \n",
    "    # precalculate the final size \n",
    "    final_size = 0\n",
    "    if augment:\n",
    "        for index, row in tqdm(dataframe.iterrows(), total=len(dataframe), desc=\"Calculating final size\"):\n",
    "            final_size += 1 \n",
    "            \n",
    "            # CUSTOM AUGMENTATION LOGIC \n",
    "            num_augmentations = 0\n",
    "            if row['Block'] == 0:\n",
    "                if row['WordRep'] == 1 or row['SoundRep'] == 1:\n",
    "                    num_augmentations = 3\n",
    "                elif row['Prolongation'] == 1:\n",
    "                    num_augmentations = 1\n",
    "            elif row['Block'] == 1:\n",
    "                if row['SoundRep'] == 1:\n",
    "                    num_augmentations = 1\n",
    "                elif row['WordRep'] == 1:\n",
    "                    num_augmentations = 1\n",
    "            \n",
    "            if row[LABEL_COLS].sum() == 1 and row['WordRep'] == 1:\n",
    "                num_augmentations += 3\n",
    "            \n",
    "            if row[LABEL_COLS].sum() == 2 and row['WordRep'] == 1 and row['SoundRep'] == 1:\n",
    "                num_augmentations += 2\n",
    "                \n",
    "            if row[LABEL_COLS].sum() == 1 and row['SoundRep'] == 1:\n",
    "                num_augmentations += 2\n",
    "            \n",
    "            final_size += num_augmentations\n",
    "    else:\n",
    "        final_size = len(dataframe)\n",
    "    \n",
    "    print(f\"Final dataset size for '{desc}' will be {final_size} samples.\")\n",
    "\n",
    "    # prealloc np arr to save mem\n",
    "    all_sequences = np.zeros((final_size, MAX_SEQ_LEN, EMBEDDING_DIM), dtype=np.float32)\n",
    "    all_labels = np.zeros((final_size, len(ALL_LABELS)), dtype=np.int8)\n",
    "    \n",
    "    current_idx = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(dataframe), BATCH_SIZE), desc=f\"{desc} Batches\"):\n",
    "            batch_df = dataframe.iloc[i:i+BATCH_SIZE]\n",
    "            \n",
    "            audio_batch = []\n",
    "            labels_batch = []\n",
    "            \n",
    "            for index, row in batch_df.iterrows():\n",
    "                clip_path = str(CLIPS_DIR / f\"{row['Clip']}.wav\")\n",
    "                audio, sr = librosa.load(clip_path, sr=SAMPLING_RATE)\n",
    "                current_labels = row[ALL_LABELS].values\n",
    "                \n",
    "                audio_batch.append(audio)\n",
    "                labels_batch.append(current_labels)\n",
    "\n",
    "                if augment:\n",
    "                    # CUSTOM AUGMENTATION LOGIC \n",
    "                    num_augmentations = 0\n",
    "                    if row['Block'] == 0:\n",
    "                        if row['WordRep'] == 1 or row['SoundRep'] == 1:\n",
    "                            num_augmentations = 3\n",
    "                        elif row['Prolongation'] == 1:\n",
    "                            num_augmentations = 1\n",
    "                    elif row['Block'] == 1:\n",
    "                        if row['SoundRep'] == 1:\n",
    "                            num_augmentations = 1\n",
    "                        elif row['WordRep'] == 1:\n",
    "                            num_augmentations = 1\n",
    "\n",
    "                    if row[LABEL_COLS].sum() == 1 and row['WordRep'] == 1:\n",
    "                        num_augmentations += 3\n",
    "                    \n",
    "                    if row[LABEL_COLS].sum() == 2 and row['WordRep'] == 1 and row['SoundRep'] == 1:\n",
    "                        num_augmentations += 2\n",
    "\n",
    "                    if row[LABEL_COLS].sum() == 1 and row['SoundRep'] == 1:\n",
    "                        num_augmentations += 2\n",
    "\n",
    "                    for _ in range(num_augmentations):\n",
    "                        augmented_audio = augmenter(samples=audio, sample_rate=SAMPLING_RATE)\n",
    "                        audio_batch.append(augmented_audio)\n",
    "                        labels_batch.append(current_labels)\n",
    "\n",
    "            inputs = processor(audio_batch, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\", padding=True)\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            sequences = model(**inputs).last_hidden_state.cpu().numpy()\n",
    "            \n",
    "            # fills\n",
    "            for j, seq in enumerate(sequences):\n",
    "                if current_idx >= final_size: break\n",
    "                if seq.shape[0] < MAX_SEQ_LEN:\n",
    "                    pad_width = MAX_SEQ_LEN - seq.shape[0]\n",
    "                    seq = np.pad(seq, ((0, pad_width), (0, 0)), mode='constant')\n",
    "                else:\n",
    "                    seq = seq[:MAX_SEQ_LEN, :]\n",
    "                \n",
    "                all_sequences[current_idx] = seq\n",
    "                all_labels[current_idx] = labels_batch[j]\n",
    "                current_idx += 1\n",
    "            if current_idx >= final_size: break\n",
    "\n",
    "    return all_sequences, all_labels\n",
    "\n",
    "print(\"\\n Starting Wav2Vec2 sequence extraction... \")\n",
    "X_train, y_train = extract_wav2vec_sequences(train_df, desc=\"Training\", augment=True)\n",
    "np.savez_compressed(TRAIN_DATA_PATH, x=X_train, y=y_train)\n",
    "print(f\"Saved compressed training data with shape: {X_train.shape}\")\n",
    "\n",
    "# distribution after augm \n",
    "final_train_labels_df = pd.DataFrame(y_train, columns=ALL_LABELS)\n",
    "print(\"\\n Final Label Distribution in Training Set (After Augmentation) \")\n",
    "print(final_train_labels_df.sum().sort_values(ascending=False))\n",
    "\n",
    "del X_train, y_train\n",
    "\n",
    "X_val, y_val = extract_wav2vec_sequences(val_df, desc=\"Validation\", augment=False)\n",
    "np.savez_compressed(VAL_DATA_PATH, x=X_val, y=y_val)\n",
    "print(f\"Saved compressed validation data with shape: {X_val.shape}\")\n",
    "del X_val, y_val\n",
    "\n",
    "X_test, y_test = extract_wav2vec_sequences(test_df, desc=\"Test\", augment=False)\n",
    "np.savez_compressed(TEST_DATA_PATH, x=X_test, y=y_test)\n",
    "print(f\"Saved compressed test data with shape: {X_test.shape}\")\n",
    "del X_test, y_test\n",
    "\n",
    "print(f\"\\n Feature extraction complete. All files saved in '{OUTPUT_DIR}' directory. \")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2285103,
     "sourceId": 3839191,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
